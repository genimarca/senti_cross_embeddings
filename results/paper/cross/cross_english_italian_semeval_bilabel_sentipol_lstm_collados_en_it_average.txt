Begin: Loading training corpus
End: Loading training corpus
Begin: Loading evaluation corpus
End: Loading evaluation corpus
Begin: Loading embeddings
End: Loading embeddings
[[173312, 50909, 317015, 354148, 382932, 6562, 460545, 255363, 124724, 161233, 415292, 377955, 242070, 0, 317015, 359100, 183581, 29378, 0, 1, 1, 1], [249634, 0, 317015, 16626, 288281, 178947, 454821, 179115, 377955, 370922, 132227, 149928, 0, 317015, 174756, 377955, 0, 1, 1, 1, 1, 1], [483590, 494386, 0, 51927, 486652, 29378, 386656, 392111, 0, 486652, 493739, 35387, 382932, 0, 186167, 215494, 212314, 0, 469068, 132227, 0, 29378], [483590, 259105, 382386, 168269, 77698, 290158, 346553, 376892, 0, 255363, 387546, 460545, 255363, 0, 302916, 0, 377955, 0, 433667, 405803, 29378, 1], [0, 149928, 0, 50909, 317015, 149502, 173311, 100612, 0, 382932, 236537, 350931, 0, 0, 132227, 144942, 7004, 495694, 189245, 0, 292692, 255363]]
Begin: Training
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 22)                0         
_________________________________________________________________
embedding_1 (Embedding)      (None, 22, 100)           50000200  
_________________________________________________________________
bidirectional_1 (Bidirection (None, 22, 256)           234496    
_________________________________________________________________
dense_1 (Dense)              (None, 22, 64)            16448     
_________________________________________________________________
dropout_1 (Dropout)          (None, 22, 64)            0         
_________________________________________________________________
dense_2 (Dense)              (None, 22, 32)            2080      
_________________________________________________________________
dropout_2 (Dropout)          (None, 22, 32)            0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 704)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 2)                 1410      
=================================================================
Total params: 50,254,634
Trainable params: 254,434
Non-trainable params: 50,000,200
_________________________________________________________________
End: Training
Begin: Loading embeddings
End: Loading embeddings
[[367480, 67270, 483590, 447249, 353931, 396512, 0, 364051, 0, 0, 0, 367480, 67270, 0, 2240, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 270386, 0, 364735, 0, 432915, 169913, 0, 495694, 0, 310567, 483590, 483590, 0, 1, 1, 1, 1, 1], [367480, 67270, 483590, 447249, 353931, 396512, 0, 364051, 0, 0, 0, 367480, 67270, 204226, 483590, 0, 1, 1, 1, 1, 1, 1], [483590, 0, 277274, 169913, 0, 0, 0, 11279, 277274, 0, 0, 0, 291191, 0, 117128, 382932, 0, 2240, 5614, 0, 0, 450855], [483590, 370638, 0, 0, 12725, 12570, 0, 449676, 222311, 0, 0, 483590, 483590, 0, 1, 1, 1, 1, 1, 1, 1, 1]]
Begin: Evaluation

  32/1050 [..............................] - ETA: 7s
 128/1050 [==>...........................] - ETA: 2s
 256/1050 [======>.......................] - ETA: 1s
 352/1050 [=========>....................] - ETA: 0s
 448/1050 [===========>..................] - ETA: 0s
 544/1050 [==============>...............] - ETA: 0s
 640/1050 [=================>............] - ETA: 0s
 736/1050 [====================>.........] - ETA: 0s
 864/1050 [=======================>......] - ETA: 0s
 992/1050 [===========================>..] - ETA: 0s
1050/1050 [==============================] - 1s 756us/step
End: Test
Train_size:	 3957
Train_size NEGATIVE:	863
Train_size POSITIVE:	3094


Eva_size:	 1050
Eva_size NEGATIVE:	734
Eva_size POSITIVE:	316

	Pred_NEGATIVE	Pred_POSITIVE
R_NEGATIVE	121	613
R_POSITIVE	37	279

Prec. NEGATIVE: 0.7658227848101266
Prec. POSITIVE: 0.312780269058296
Recall NEGATIVE: 0.164850136239782
Recall POSITIVE: 0.8829113924050633
F1 NEGATIVE: 0.2713004484304933
F1 POSITIVE: 0.46192052980132453
Macro-Precision: 0.5393015269342113
Macro-Recall: 0.5238807643224227
Macro-F1: 0.3666104891159089
Accuracy: 0.38095238095238093

GoodBye

